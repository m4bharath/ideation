from airflow import DAG
from airflow.operators.ssh import SSHOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import BranchPythonOperator
from airflow.utils.dates import days_ago

default_args = {
    'owner': 'your_owner',
    'start_date': days_ago(2),
    'params': {
        'skip_stage_load': 'N'  # Default value
    }
}

with DAG(
    'write_data_to_stage_dag',
    default_args=default_args,
    schedule_interval=None
) as dag:

    skip_stage_flag = dag.params.skip_stage_load

    with TaskGroup("tg_write_stage_load") as tg_write_stage_load:
        t_write_data_to_stage = SSHOperator(
            task_id='t_stg_load',
            ssh_conn_id='ATLAS SSH',
            command="source/home/{{ ti.xcom_pull(key='metainfo') }}/statsenv; your_actual_command",
            trigger_rule="all_success"
        )

        t_dummy_task = DummyOperator(
            task_id='t_dummy_task'
        )

        # Branching within the task group
        branching = BranchPythonOperator(
            task_id='branching',
            python_callable=lambda: 't_dummy_task' if skip_stage_flag == 'Y' else 't_stg_load',
            dag=tg_write_stage_load
        )

        # Set both tasks as downstream of the branching operator within the task group
        t_write_data_to_stage >> branching
        t_dummy_task >> branching

    # Set the task group as upstream for the subsequent task
    tg_write_stage_load >> subsequent_task
